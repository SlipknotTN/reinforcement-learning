{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tile Coding\n",
    "---\n",
    "\n",
    "Tile coding is an innovative way of discretizing a continuous space that enables better generalization compared to a single grid-based approach. The fundamental idea is to create several overlapping grids or _tilings_; then for any given sample value, you need only check which tiles it lies in. You can then encode the original continuous value by a vector of integer indices or bits that identifies each activated tile.\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import common libraries\n",
    "import pickle\n",
    "import sys\n",
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set plotting options\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "np.set_printoptions(precision=3, linewidth=120)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Specify the Environment, and Explore the State and Action Spaces\n",
    "\n",
    "We'll use [OpenAI Gym](https://gym.openai.com/) environments to test and develop our algorithms. These simulate a variety of classic as well as contemporary reinforcement learning tasks.  Let's begin with an environment that has a continuous state space, but a discrete action space.\n",
    "\n",
    "Link to [Acrobot sources](https://github.com/openai/gym/blob/master/gym/envs/classic_control/acrobot.py) for a better explanations of states and actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment\n",
    "env = gym.make('Acrobot-v1')\n",
    "env.seed(505);\n",
    "\n",
    "# Explore state (observation) space\n",
    "print(\"State space:\", env.observation_space)\n",
    "print(\"- low:\", env.observation_space.low)\n",
    "print(\"- high:\", env.observation_space.high)\n",
    "\n",
    "# Explore action space\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the state space is multi-dimensional, with most dimensions ranging from -1 to 1 (positions of the two joints), while the final two dimensions have a larger range. How do we discretize such a space using tiles?\n",
    "\n",
    "### 3. Tiling\n",
    "\n",
    "Let's first design a way to create a single tiling for a given state space. This is very similar to a uniform grid! The only difference is that you should include an offset for each dimension that shifts the split points.\n",
    "\n",
    "For instance, if `low = [-1.0, -5.0]`, `high = [1.0, 5.0]`, `bins = (10, 10)`, and `offsets = (-0.1, 0.5)`, then return a list of 2 NumPy arrays (2 dimensions) each containing the following split points (9 split points per dimension):\n",
    "\n",
    "```\n",
    "[array([-0.9, -0.7, -0.5, -0.3, -0.1,  0.1,  0.3,  0.5,  0.7]),\n",
    " array([-3.5, -2.5, -1.5, -0.5,  0.5,  1.5,  2.5,  3.5,  4.5])]\n",
    "```\n",
    "\n",
    "Notice how the split points for the first dimension are offset by `-0.1`, and for the second dimension are offset by `+0.5`. This might mean that some of our tiles, especially along the perimeter, are partially outside the valid state space, but that is unavoidable and harmless."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tiling_grid(low, high, bins=(10, 10), offsets=(0.0, 0.0)):\n",
    "    \"\"\"Define a uniformly-spaced grid that can be used for tile-coding a space.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    low : array_like\n",
    "        Lower bounds for each dimension of the continuous space.\n",
    "    high : array_like\n",
    "        Upper bounds for each dimension of the continuous space.\n",
    "    bins : tuple\n",
    "        Number of bins or tiles along each corresponding dimension.\n",
    "    offsets : tuple\n",
    "        Split points for each dimension should be offset by these values.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    grid : list of array_like\n",
    "        A list of arrays containing split points for each dimension.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this\n",
    "    ranges = np.array(high) - np.array(low)\n",
    "    # print(ranges)\n",
    "    bins_sizes = ranges / bins\n",
    "    # print(bins_sizes)\n",
    "    grid = []\n",
    "    for bin_dim, bin_num in enumerate(bins):\n",
    "        split_dim = np.linspace(\n",
    "                        start=low[bin_dim] + bins_sizes[bin_dim] + offsets[bin_dim],\n",
    "                        stop=high[bin_dim] + offsets[bin_dim],\n",
    "                        num=bins[bin_dim] - 1,\n",
    "                        endpoint=False\n",
    "                    )\n",
    "        grid.append(split_dim)\n",
    "    #print(grid)\n",
    "    return grid\n",
    "\n",
    "\n",
    "low = [-1.0, -5.0]\n",
    "high = [1.0, 5.0]\n",
    "create_tiling_grid(low, high, bins=(10, 10), offsets=(-0.1, 0.5))  # [test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now use this function to define a set of tilings that are a little offset from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tilings(low, high, tiling_specs):\n",
    "    \"\"\"Define multiple tilings using the provided specifications.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    low : array_like\n",
    "        Lower bounds for each dimension of the continuous space.\n",
    "    high : array_like\n",
    "        Upper bounds for each dimension of the continuous space.\n",
    "    tiling_specs : list of tuples\n",
    "        A sequence of (bins, offsets) to be passed to create_tiling_grid().\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tilings : list\n",
    "        A list of tilings (grids), each produced by create_tiling_grid().\n",
    "    \"\"\"\n",
    "    # TODO: Implement this\n",
    "    return [create_tiling_grid(low, high, bins=tiling_spec[0], offsets=tiling_spec[1]) for tiling_spec in tiling_specs]\n",
    "\n",
    "\n",
    "# Tiling specs: [(<bins>, <offsets>), ...]\n",
    "tiling_specs = [((10, 10), (-0.066, -0.33)),\n",
    "                ((10, 10), (0.0, 0.0)),\n",
    "                ((10, 10), (0.066, 0.33))]\n",
    "tilings = create_tilings(low, high, tiling_specs)\n",
    "tilings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be hard to gauge whether you are getting desired results or not. So let's try to visualize these tilings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.lines import Line2D\n",
    "\n",
    "def visualize_tilings(tilings):\n",
    "    \"\"\"Plot each tiling as a grid.\"\"\"\n",
    "    prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "    colors = prop_cycle.by_key()['color']\n",
    "    linestyles = ['-', '--', ':']\n",
    "    legend_lines = []\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    for i, grid in enumerate(tilings):\n",
    "        for x in grid[0]:\n",
    "            l = ax.axvline(x=x, color=colors[i % len(colors)], linestyle=linestyles[i % len(linestyles)], label=i)\n",
    "        for y in grid[1]:\n",
    "            l = ax.axhline(y=y, color=colors[i % len(colors)], linestyle=linestyles[i % len(linestyles)])\n",
    "        legend_lines.append(l)\n",
    "    ax.grid('off')\n",
    "    ax.legend(legend_lines, [\"Tiling #{}\".format(t) for t in range(len(legend_lines))], facecolor='white', framealpha=0.9)\n",
    "    ax.set_title(\"Tilings\")\n",
    "    return ax  # return Axis object to draw on later, if needed\n",
    "\n",
    "\n",
    "visualize_tilings(tilings);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now that we have a way to generate these tilings, we can next write our encoding function that will convert any given continuous state value to a discrete vector.\n",
    "\n",
    "### 4. Tile Encoding\n",
    "\n",
    "Implement the following to produce a vector that contains the indices for each tile that the input state value belongs to. The shape of the vector can be the same as the arrangment of tiles you have, or it can be ultimately flattened for convenience.\n",
    "\n",
    "You can use the same `discretize()` function here from grid-based discretization, and simply call it for each tiling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize(sample, grid):\n",
    "    \"\"\"Discretize a sample as per given grid.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample : array_like\n",
    "        A single sample from the (original) continuous space.\n",
    "    grid : list of array_like\n",
    "        A list of arrays containing split points for each dimension.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    discretized_sample : array_like\n",
    "        A sequence of integers with the same number of dimensions as sample.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this\n",
    "    assert len(sample) == len(grid)\n",
    "    # print(sample)\n",
    "    bin_indexes = [np.digitize(sample_dim, grid_dim) for sample_dim, grid_dim in zip(sample, grid)]\n",
    "    # print(bin_indexes)\n",
    "    return bin_indexes\n",
    "\n",
    "\n",
    "def tile_encode(sample, tilings, flatten=False):\n",
    "    \"\"\"Encode given sample using tile-coding.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    sample : array_like\n",
    "        A single sample from the (original) continuous space.\n",
    "    tilings : list\n",
    "        A list of tilings (grids), each produced by create_tiling_grid().\n",
    "    flatten : bool\n",
    "        If true, flatten the resulting binary arrays into a single long vector.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    encoded_sample : list or array_like\n",
    "        A list of binary vectors, one for each tiling, or flattened into one.\n",
    "    \"\"\"\n",
    "    # TODO: Implement this\n",
    "    sample_encodings = [discretize(sample, tiling) for tiling in tilings]\n",
    "    # print(sample_encodings)\n",
    "    if flatten:\n",
    "        flatten_encodings = []\n",
    "        for sample_encoding in sample_encodings:\n",
    "            flatten_encodings.extend(sample_encoding)\n",
    "        return flatten_encodings\n",
    "    return sample_encodings\n",
    "\n",
    "\n",
    "# Test with some sample values\n",
    "samples = [(-1.2 , -5.1 ),\n",
    "           (-0.75,  3.25),\n",
    "           (-0.5 ,  0.0 ),\n",
    "           ( 0.25, -1.9 ),\n",
    "           ( 0.15, -1.75),\n",
    "           ( 0.75,  2.5 ),\n",
    "           ( 0.7 , -3.7 ),\n",
    "           ( 1.0 ,  5.0 )]\n",
    "encoded_samples = [tile_encode(sample, tilings, flatten=False) for sample in samples]\n",
    "print(\"\\nSamples:\", repr(samples), sep=\"\\n\")\n",
    "print(\"\\nEncoded samples:\", repr(encoded_samples), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we did not flatten the encoding above, which is why each sample's representation is a pair of indices for each tiling. This makes it easy to visualize it using the tilings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "def visualize_encoded_samples(samples, encoded_samples, tilings, low=None, high=None):\n",
    "    \"\"\"Visualize samples by activating the respective tiles.\"\"\"\n",
    "    samples = np.array(samples)  # for ease of indexing\n",
    "\n",
    "    # Show tiling grids\n",
    "    ax = visualize_tilings(tilings)\n",
    "    \n",
    "    # If bounds (low, high) are specified, use them to set axis limits\n",
    "    if low is not None and high is not None:\n",
    "        ax.set_xlim(low[0], high[0])\n",
    "        ax.set_ylim(low[1], high[1])\n",
    "    else:\n",
    "        # Pre-render (invisible) samples to automatically set reasonable axis limits, and use them as (low, high)\n",
    "        ax.plot(samples[:, 0], samples[:, 1], 'o', alpha=0.0)\n",
    "        low = [ax.get_xlim()[0], ax.get_ylim()[0]]\n",
    "        high = [ax.get_xlim()[1], ax.get_ylim()[1]]\n",
    "\n",
    "    # Map each encoded sample (which is really a list of indices) to the corresponding tiles it belongs to\n",
    "    tilings_extended = [np.hstack((np.array([low]).T, grid, np.array([high]).T)) for grid in tilings]  # add low and high ends\n",
    "    tile_centers = [(grid_extended[:, 1:] + grid_extended[:, :-1]) / 2 for grid_extended in tilings_extended]  # compute center of each tile\n",
    "    tile_toplefts = [grid_extended[:, :-1] for grid_extended in tilings_extended]  # compute topleft of each tile\n",
    "    tile_bottomrights = [grid_extended[:, 1:] for grid_extended in tilings_extended]  # compute bottomright of each tile\n",
    "\n",
    "    prop_cycle = plt.rcParams['axes.prop_cycle']\n",
    "    colors = prop_cycle.by_key()['color']\n",
    "    for sample, encoded_sample in zip(samples, encoded_samples):\n",
    "        for i, tile in enumerate(encoded_sample):\n",
    "            # Shade the entire tile with a rectangle\n",
    "            topleft = tile_toplefts[i][0][tile[0]], tile_toplefts[i][1][tile[1]]\n",
    "            bottomright = tile_bottomrights[i][0][tile[0]], tile_bottomrights[i][1][tile[1]]\n",
    "            ax.add_patch(Rectangle(topleft, bottomright[0] - topleft[0], bottomright[1] - topleft[1],\n",
    "                                   color=colors[i], alpha=0.33))\n",
    "\n",
    "            # In case sample is outside tile bounds, it may not have been highlighted properly\n",
    "            if any(sample < topleft) or any(sample > bottomright):\n",
    "                # So plot a point in the center of the tile and draw a connecting line\n",
    "                cx, cy = tile_centers[i][0][tile[0]], tile_centers[i][1][tile[1]]\n",
    "                ax.add_line(Line2D([sample[0], cx], [sample[1], cy], color=colors[i]))\n",
    "                ax.plot(cx, cy, 's', color=colors[i])\n",
    "    \n",
    "    # Finally, plot original samples\n",
    "    ax.plot(samples[:, 0], samples[:, 1], 'o', color='r')\n",
    "\n",
    "    ax.margins(x=0, y=0)  # remove unnecessary margins\n",
    "    ax.set_title(\"Tile-encoded samples\")\n",
    "    return ax\n",
    "\n",
    "visualize_encoded_samples(samples, encoded_samples, tilings);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspect the results and make sure you understand how the corresponding tiles are being chosen. Note that some samples may have one or more tiles in common.\n",
    "\n",
    "### 5. Q-Table with Tile Coding\n",
    "\n",
    "The next step is to design a special Q-table that is able to utilize this tile coding scheme. It should have the same kind of interface as a regular table, i.e. given a `<state, action>` pair, it should return a `<value>`. Similarly, it should also allow you to update the `<value>` for a given `<state, action>` pair (note that this should update all the tiles that `<state>` belongs to).\n",
    "\n",
    "The `<state>` supplied here is assumed to be from the original continuous state space, and `<action>` is discrete (and integer index). The Q-table should internally convert the `<state>` to its tile-coded representation when required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QTable:\n",
    "    \"\"\"Simple Q-table.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size):\n",
    "        \"\"\"Initialize Q-table.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state_size : tuple\n",
    "            Number of discrete values along each dimension of state space.\n",
    "        action_size : int\n",
    "            Number of discrete actions in action space.\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "\n",
    "        # print(state_size)\n",
    "        # print(action_size)\n",
    "        # TODO: Create Q-table, initialize all Q-values to zero\n",
    "        # Note: If state_size = (9, 9), action_size = 2, q_table.shape should be (9, 9, 2)\n",
    "        self.q_table = np.zeros(tuple(state_size) + (action_size,), dtype=np.float32)\n",
    "        print(\"QTable(): size =\", self.q_table.shape)\n",
    "\n",
    "\n",
    "class TiledQTable:\n",
    "    \"\"\"Composite Q-table with an internal tile coding scheme.\"\"\"\n",
    "    \n",
    "    def __init__(self, low, high, tiling_specs, action_size):\n",
    "        \"\"\"Create tilings and initialize internal Q-table(s).\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        low : array_like\n",
    "            Lower bounds for each dimension of state space.\n",
    "        high : array_like\n",
    "            Upper bounds for each dimension of state space.\n",
    "        tiling_specs : list of tuples\n",
    "            A sequence of (bins, offsets) to be passed to create_tilings() along with low, high.\n",
    "        action_size : int\n",
    "            Number of discrete actions in action space.\n",
    "        \"\"\"\n",
    "        self.tilings = create_tilings(low, high, tiling_specs)\n",
    "        # print(self.tilings)\n",
    "        self.state_sizes = [tuple(len(splits)+1 for splits in tiling_grid) for tiling_grid in self.tilings]\n",
    "        # print(self.state_sizes)\n",
    "        self.action_size = action_size\n",
    "        self.q_tables = [QTable(state_size, self.action_size) for state_size in self.state_sizes]\n",
    "        print(\"TiledQTable(): no. of internal tables = \", len(self.q_tables))\n",
    "    \n",
    "    def get_state_action(self, state, action):\n",
    "        \"\"\"Get Q-value for given <state, action> pair.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : array_like\n",
    "            Vector representing the state in the original continuous space.\n",
    "        action : int\n",
    "            Index of desired action.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        value : float\n",
    "            Q-value of given <state, action> pair, averaged from all internal Q-tables.\n",
    "        \"\"\"\n",
    "        # TODO: Encode state to get tile indices\n",
    "        # print(state)\n",
    "        encodings_state = tile_encode(state, self.tilings)\n",
    "        # print(encodings_state)\n",
    "        # TODO: Retrieve q-value for each tiling, and return their average\n",
    "        q_values = [self.q_tables[tile_ix].q_table[tuple(encoding_state) + (action,)] \\\n",
    "                    for tile_ix, encoding_state in enumerate(encodings_state)]\n",
    "        return np.mean(q_values)\n",
    "    \n",
    "    def get_state(self, state):\n",
    "        \"\"\"Get Q-values for given <state> pair.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : array_like\n",
    "            Vector representing the state in the original continuous space.\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        value : array_like\n",
    "            Q-values of given state from all internal Q-tables.\n",
    "        \"\"\"\n",
    "        # TODO: Encode state to get tile indices\n",
    "        # print(state)\n",
    "        encodings_state = tile_encode(state, self.tilings)\n",
    "        # print(encodings_state)\n",
    "        # TODO: Retrieve q-value for each tiling, and return their average\n",
    "        q_values = [self.q_tables[tile_ix].q_table[tuple(encoding_state) + (action,)] \\\n",
    "                    for tile_ix, encoding_state in enumerate(encodings_state)]\n",
    "        return q_values\n",
    "    \n",
    "    def update(self, state, action, value, alpha=0.1):\n",
    "        \"\"\"Soft-update Q-value for given <state, action> pair to value.\n",
    "        \n",
    "        Instead of overwriting Q(state, action) with value, perform soft-update:\n",
    "            Q(state, action) = alpha * value + (1.0 - alpha) * Q(state, action)\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        state : array_like\n",
    "            Vector representing the state in the original continuous space.\n",
    "        action : int\n",
    "            Index of desired action.\n",
    "        value : float\n",
    "            Desired Q-value for <state, action> pair.\n",
    "        alpha : float\n",
    "            Update factor to perform soft-update, in [0.0, 1.0] range.\n",
    "        \"\"\"\n",
    "        # TODO: Encode state to get tile indices\n",
    "        encodings_state = tile_encode(state, self.tilings)\n",
    "        # TODO: Update q-value for each tiling by update factor alpha\n",
    "        for tile_ix, q_table in enumerate(self.q_tables):\n",
    "            old_value = q_table.q_table[tuple(encodings_state[tile_ix]) + (action,)]\n",
    "            q_table.q_table[tuple(encodings_state[tile_ix]) + (action,)] = \\\n",
    "                alpha * value + (1.0 - alpha) * old_value\n",
    "\n",
    "\n",
    "# Test with a sample Q-table\n",
    "tq = TiledQTable(low, high, tiling_specs, action_size=2)\n",
    "s1 = 3; s2 = 4; a = 0; q = 1.0\n",
    "print(\"[GET]    Q({}, {}) = {}\".format(samples[s1], a, tq.get_state_action(samples[s1], a)))  # check value at sample = s1, action = a\n",
    "print(\"[UPDATE] Q({}, {}) = {}\".format(samples[s2], a, q)); tq.update(samples[s2], a, q)  # update value for sample with some common tile(s)\n",
    "print(\"[GET]    Q({}, {}) = {}\".format(samples[s1], a, tq.get_state_action(samples[s1], a)))  # check value again, should be slightly updated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you update the q-value for a particular state (say, `(0.25, -1.91)`) and action (say, `0`), then you should notice the q-value of a nearby state (e.g. `(0.15, -1.75)` and same action) has changed as well! This is how tile-coding is able to generalize values across the state space better than a single uniform grid."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Implement a Q-Learning Agent using Tile-Coding\n",
    "\n",
    "Now it's your turn to apply this discretization technique to design and test a complete learning agent! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an environment\n",
    "env = gym.make('Acrobot-v1')\n",
    "\n",
    "env_low = env.observation_space.low\n",
    "env_high = env.observation_space.high\n",
    "\n",
    "# Explore state (observation) space\n",
    "print(\"State space:\", env.observation_space)\n",
    "print(\"- low:\", env_low)\n",
    "print(\"- high:\", env_high)\n",
    "\n",
    "# Explore action space\n",
    "print(\"Action space:\", env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = env.reset()\n",
    "score = 0\n",
    "for t in range(200):\n",
    "    action = env.action_space.sample()\n",
    "    env.render()\n",
    "    state, reward, done, _ = env.step(action)\n",
    "    score += reward\n",
    "    if done:\n",
    "        break \n",
    "print('Final score:', score)\n",
    "env.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "QLearningAgent Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"Q-Learning agent that can act on a continuous state space by discretizing it.\"\"\"\n",
    "\n",
    "    def __init__(self, env, tiled_qtable, alpha=0.02, gamma=0.99,\n",
    "                 start_epsilon=1.0, epsilon_decay_rate=0.99, min_epsilon=0.01):\n",
    "        self.env = env\n",
    "        self.action_size = self.env.action_space.n\n",
    "        self.tiled_qtable = tiled_qtable\n",
    "        self.alpha = alpha\n",
    "        # discount factor\n",
    "        self.gamma = gamma\n",
    "        # epsilon\n",
    "        self.epsilon = start_epsilon\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate\n",
    "        self.min_epsilon = min_epsilon\n",
    "        # Last state for every q_table\n",
    "        self.last_state = None\n",
    "        # Last state (encoded) for every q_table\n",
    "        self.last_encodings_state = None\n",
    "        # Last taken action\n",
    "        self.last_action = None\n",
    "    \n",
    "    def get_best_action(self, encodings_state):\n",
    "        # VOTING\n",
    "        best_actions = []\n",
    "        for tile_ix, encoding_state in enumerate(encodings_state):\n",
    "            # print(self.tiled_qtable.q_tables[tile_ix].q_table[tuple(encoding_state)])\n",
    "            tile_best_action = np.argmax(self.tiled_qtable.q_tables[tile_ix].q_table[tuple(encoding_state)])\n",
    "            best_actions.append(tile_best_action)            \n",
    "        # Vote the most frequent action\n",
    "        return np.bincount(best_actions).argmax()\n",
    "    \n",
    "        # AVG + MAX\n",
    "        # return np.argmax(self.tiled_qtable.get(state, action)\n",
    "    \n",
    "    def get_q_value(self, state, action):\n",
    "        return self.tiled_qtable.get_state_action(state, action)\n",
    "    \n",
    "    def reset_episode(self, state):\n",
    "        # Update epsilon\n",
    "        self.epsilon *= self.epsilon_decay_rate\n",
    "        self.epsilon = max(self.epsilon, self.min_epsilon)\n",
    "        # Reset last_state and last_action\n",
    "        self.last_state = state\n",
    "        self.last_encodings_state = tile_encode(state, self.tiled_qtable.tilings)\n",
    "        # Caveat: this action is not really taken, we start with the best action for initial state\n",
    "        self.last_action = self.get_best_action(self.last_encodings_state)\n",
    "        return self.last_action\n",
    "    \n",
    "    def get_action(self, state, reward, mode=\"train\"):\n",
    "        assert self.last_state is not None\n",
    "        assert self.last_action is not None\n",
    "        # Get encodings for every tiling\n",
    "        encodings_state = tile_encode(state, self.tiled_qtable.tilings)\n",
    "        # Get best action for every tiling\n",
    "        # print(encodings_state)\n",
    "        if mode == \"test\":\n",
    "            return self.get_best_action(encodings_state)\n",
    "        elif mode == \"train\":\n",
    "            # TODO: Learn Q considering reward for last_state and last_action\n",
    "            \n",
    "            # New component for Q: reward + gamma * max_a(Q(S_t+1, a))\n",
    "            # state == S_t+1\n",
    "            # last_state == S_t\n",
    "            # last_action == a_t\n",
    "            # take_action == a_t+1\n",
    "                \n",
    "            # Get avg Q value\n",
    "            new_value = reward + self.gamma * max(self.tiled_qtable.get_state(state=state))  \n",
    "            # Q(S_t, a_t) = alpha * new_value + (1 - alpha) * old_value\n",
    "            \n",
    "            # Update all the tiles\n",
    "            self.tiled_qtable.update(\n",
    "                state=self.last_state,\n",
    "                action=self.last_action,\n",
    "                value=new_value,\n",
    "                alpha=self.alpha\n",
    "            )\n",
    "            \n",
    "            if np.random.uniform(0, 1) < self.epsilon:\n",
    "                # Exploration -> select a random action among random actions (for every tile)\n",
    "                take_action = np.random.randint(0, self.action_size)\n",
    "            else:\n",
    "                # Exploitation -> select the most voted action\n",
    "                take_action = self.get_best_action(encodings_state)\n",
    "            \n",
    "            self.last_action = take_action\n",
    "            self.last_state = state\n",
    "            self.last_encodings_state = encodings_state\n",
    "            return take_action\n",
    "        else:\n",
    "            raise Exception(f\"Mode {mode} unknown\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and test functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(env, agent, num_episodes=10000, test_steps=50, mode=\"train\", test_with_gui=False):\n",
    "    print(\"Running...\")\n",
    "    scores = []\n",
    "    \n",
    "    for num_episode in range(num_episodes):\n",
    "        init_state = env.reset()\n",
    "        done = False\n",
    "        # We take always the exploitation action as first action,\n",
    "        # to start with a consistent last_action, last_state and reward triplet\n",
    "        # The reward depends on init_state + first_action and we end at state\n",
    "        action = agent.reset_episode(init_state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        score = reward\n",
    "        while not done:\n",
    "            action = agent.get_action(state, reward=reward, mode=mode)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            score += reward\n",
    "        \n",
    "        # The score is the cumulative reward\n",
    "        scores.append(score)\n",
    "        \n",
    "        if (num_episode + 1) % test_steps == 0:\n",
    "            if mode == \"train\":\n",
    "                with open(f\"agent_{num_episode + 1}.pkl\", \"wb\") as file_out:\n",
    "                    pickle.dump(agent, file_out)\n",
    "            print(f\"Num episode {num_episode + 1} / {num_episodes}, \" + \\\n",
    "                  f\"Avg score last {test_steps} runs: {np.mean(scores[-test_steps:])}\")\n",
    "            print(f\"Reached epsilon: {agent.epsilon}\")\n",
    "            if test_with_gui:\n",
    "                run_test_episode_with_gui(env, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test_episode_with_gui(env, agent):\n",
    "    state = env.reset()\n",
    "    env.render()\n",
    "    done = False\n",
    "    score = 0\n",
    "    while not done:\n",
    "        action = agent.get_action(state, 0.0, \"test\")\n",
    "        state, reward, done, info = env.step(action)\n",
    "        env.render()\n",
    "        score += reward\n",
    "    print(f\"Episode score: {score}\")\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agent initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = (10, 10, 10, 10, 20, 20)\n",
    "start_epsilon = 1.0  # Probability to do exploration instead of exploitation.\n",
    "epsilon_decay_rate = 0.998 # E.g. 0.995^800 * 1.0 = 0.018, so about 800-900 episodes to get the minimum epsilon \n",
    "min_epsilon = 0.01\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 0.99  # Discount rate\n",
    "num_episodes = 20000\n",
    "env_tiling_specs = [\n",
    "    (bins, (-0.1, -0.1, -0.1, -0.1, -1.0, -1.0)),\n",
    "    (bins, (0.0, 0.0, 0.0, 0.0, 0.0, 0.0)),\n",
    "    (bins, (0.1, 0.1, 0.1, 0.1, 1.0, 1.0))\n",
    "]\n",
    "env_tqt = TiledQTable(\n",
    "    low=env_low,\n",
    "    high=env_high,\n",
    "    tiling_specs=env_tiling_specs,\n",
    "    action_size=int(env.action_space.n))\n",
    "agent = QLearningAgent(\n",
    "    env,\n",
    "    env_tqt,\n",
    "    alpha=alpha,\n",
    "    start_epsilon=start_epsilon,\n",
    "    epsilon_decay_rate=epsilon_decay_rate,\n",
    "    min_epsilon=min_epsilon,\n",
    "    gamma=gamma\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "run(env, agent, num_episodes, test_steps=100, mode=\"train\", test_with_gui=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try the trained agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a model from pkl if you want to reuse an old model\n",
    "run_test_episode_with_gui(env, agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the agent (set num_episodes == test_steps to get the average score over all episodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run(env, agent, num_episodes=1000, test_steps=1000, mode=\"test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
